# -*- coding: utf-8 -*-
"""final deep learning project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7d7A3KXMfLFlw4LQzHRHS_GeoRqcZHp
"""

from google.colab import drive
drive.mount('/content/drive')

import os
from glob import glob
import cv2
import matplotlib.pyplot as plt

# Paths to CT scan folders
ct_stroke_path = "/content/drive/MyDrive/ct_scans/Stroke"  # Stroke CT scans
ct_normal_path = "/content/drive/MyDrive/ct_scans/Normal"  # Normal CT scans

# Fetch all image file paths
stroke_images = sorted(glob(os.path.join(ct_stroke_path, "*.jpg")))  # Change to "*.png" if needed
normal_images = sorted(glob(os.path.join(ct_normal_path, "*.jpg")))

# Check if images are found
if not stroke_images or not normal_images:
    print("No image files found in one or both folders. Please check the paths and file types.")
else:
    print(f"Found {len(stroke_images)} stroke images and {len(normal_images)} normal images.")

    # Display one sample image from each category
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Stroke sample image
    stroke_sample = cv2.imread(stroke_images[0], cv2.IMREAD_GRAYSCALE)
    axes[0].imshow(stroke_sample, cmap='gray')
    axes[0].set_title("Stroke CT Scan")
    axes[0].axis("off")

    # Normal sample image
    normal_sample = cv2.imread(normal_images[0], cv2.IMREAD_GRAYSCALE)
    axes[1].imshow(normal_sample, cmap='gray')
    axes[1].set_title("Normal CT Scan")
    axes[1].axis("off")

    plt.show()

"""# Preprocessing images"""

import cv2
import numpy as np
from glob import glob
import os
from concurrent.futures import ThreadPoolExecutor

# Define image size
IMG_SIZE = (224, 224)

# Paths to CT scan folders
ct_stroke_path = "/content/drive/MyDrive/ct_scans/Stroke"
ct_normal_path = "/content/drive/MyDrive/ct_scans/Normal"

# Get all image paths
stroke_images = sorted(glob(os.path.join(ct_stroke_path, "*.jpg")))
normal_images = sorted(glob(os.path.join(ct_normal_path, "*.jpg")))

# Function to preprocess a single image
def preprocess_ct(image_path):
    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Read image as is
    if image is None:
        return None  # Skip invalid images

    image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_AREA)  # Resize efficiently
    image = image.astype(np.float32) / 255.0  # Normalize pixel values to [0,1]
    return np.expand_dims(image, axis=-1)  # Add channel dimension

# Process images in parallel
with ThreadPoolExecutor() as executor:
    X_data = list(executor.map(preprocess_ct, stroke_images + normal_images))

# Filter out any failed loads
X_data = np.array([img for img in X_data if img is not None], dtype=np.float32)

# Create labels
y_labels = np.array([1] * len(stroke_images) + [0] * len(normal_images), dtype=np.int32)

# Print dataset shape
print(f"Total CT Images: {X_data.shape[0]}, Image Shape: {X_data.shape[1:]}, Labels: {len(y_labels)}")

from sklearn.model_selection import train_test_split

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, random_state=42)

# Further split test set into validation (10%) and test (10%)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

"""# Basic CNN Model"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

# Define CNN model
def build_basic_cnn(input_shape=(224, 224, 1)):
    model = Sequential([
        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2,2)),

        Conv2D(64, (3,3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2,2)),

        Conv2D(128, (3,3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2,2)),

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(2, activation='softmax')  # Output layer (Stroke vs. Normal)
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Initialize model
cnn_model = build_basic_cnn()

# Print model summary
cnn_model.summary()

"""# EfficientNet-B7"""

# ✅ Mount Google Drive
from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

drive.mount('/content/drive')

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # EfficientNet-B7 requires 600x600 input size
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load EfficientNet-B7 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.efficientnet_b7(pretrained=True)
model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_efficientnet_b7.pth")
print("✅ Model saved successfully!")

"""

# ResNet50"""

# ✅ Mount Google Drive
from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

drive.mount('/content/drive')

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load ResNet50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # Adjust for 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix Function
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes  # Get class names from dataset

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)

# ✅ Plot Confusion Matrix
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_resnet50.pth")
print("✅ Model saved successfully!")

"""# Swim Transform"""

# ✅ Mount Google Drive
from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from timm import create_model

drive.mount('/content/drive')

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Swin requires 224x224 input size
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load Swin Transformer Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = create_model("swin_tiny_patch4_window7_224", pretrained=True, num_classes=2)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_swin_transformer.pth")
print("✅ Model saved successfully!")

from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from timm import create_model

drive.mount('/content/drive')

data_dir = "/content/drive/MyDrive/ct_scans"

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = create_model("swin_tiny_patch4_window7_224", pretrained=True, num_classes=2)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50)

plot_confusion_matrix(model, val_loader)

torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_swin_transformer.pth")
print("✅ Model saved successfully!")

"""# Grad Cam Added for swim transform"""

# ✅ Grad-CAM for Swin Transformer
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms
from timm import create_model

# ✅ Load the trained Swin Transformer model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = create_model("swin_tiny_patch4_window7_224", pretrained=False, num_classes=2)
model.load_state_dict(torch.load("/content/drive/MyDrive/ctscan_swin_transformer.pth"))
model = model.to(device)
model.eval()

# ✅ Define Grad-CAM Class
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self.hook_layers()

    def hook_layers(self):
        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0]

        def forward_hook(module, input, output):
            self.activations = output

        self.target_layer.register_forward_hook(forward_hook)
        self.target_layer.register_backward_hook(backward_hook)

    def generate(self, input_image, class_idx):
        input_image = input_image.to(device).unsqueeze(0)
        output = self.model(input_image)
        self.model.zero_grad()
        output[0, class_idx].backward()

        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])

        for i in range(self.activations.shape[1]):
            self.activations[:, i, :, :] *= pooled_gradients[i]

        heatmap = torch.mean(self.activations, dim=1).squeeze().cpu().detach().numpy()
        heatmap = np.maximum(heatmap, 0)
        heatmap = cv2.resize(heatmap, (224, 224))
        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))
        return heatmap

# ✅ Load an Example CT Scan Image
def load_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_tensor = transform(image)
    return image, image_tensor

# ✅ Apply Grad-CAM
def apply_gradcam(image_path):
    image, image_tensor = load_image(image_path)
    gradcam = GradCAM(model, model.layers[-1])  # Last Swin layer

    # Predict the class
    output = model(image_tensor.to(device).unsqueeze(0))
    class_idx = torch.argmax(output).item()

    heatmap = gradcam.generate(image_tensor, class_idx)

    # Overlay Grad-CAM heatmap on the image
    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)
    overlay = cv2.addWeighted(image, 0.6, heatmap, 0.4, 0)

    # Display results
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(image)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(overlay)
    plt.title("Grad-CAM Overlay")
    plt.axis("off")
    plt.show()

# ✅ Run Grad-CAM on an Example Image
example_image_path = "/content/drive/MyDrive/ct_scans/Normal/100 (1).jpg"  # Update with a real image path
apply_gradcam(example_image_path)

"""# ConvNeXt"""

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Import Required Libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load ConvNeXt Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.convnext_base(pretrained=True)
model.classifier[2] = nn.Linear(model.classifier[2].in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_convnext.pth")
print("✅ Model saved successfully!")

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Import Required Libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load ConvNeXt Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.convnext_base(pretrained=True)
model.classifier[2] = nn.Linear(model.classifier[2].in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_convnext.pth")
print("✅ Model saved successfully!")

"""# DenseNet"""

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Import Required Libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# ✅ Load DenseNet Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.densenet201(pretrained=True)
model.classifier = nn.Linear(model.classifier.in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# ✅ Training Function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):
    train_acc_list, val_acc_list = [], []
    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Model
torch.save(model.state_dict(), "/content/drive/MyDrive/ctscan_densenet.pth")
print("✅ Model saved successfully!")

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Import Required Libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from torch.cuda.amp import autocast, GradScaler

# ✅ Clear GPU Memory (If Restarting)
torch.cuda.empty_cache()

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# ✅ Load DenseNet Model (Optimized to DenseNet121)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)
model.classifier = nn.Linear(model.classifier.in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
scaler = GradScaler()  # Mixed precision training

# ✅ Training Function with Mixed Precision
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, save_path="/content/drive/MyDrive/ctscan_densenet.pth"):
    train_acc_list, val_acc_list = [], []

    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            with autocast():  # Enable Mixed Precision
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

        # ✅ Save Model Every Epoch (In case of crash)
        torch.save(model.state_dict(), save_path)

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Load Model if Exists (Resume Training)
model_path = "/content/drive/MyDrive/ctscan_densenet.pth"
try:
    model.load_state_dict(torch.load(model_path))
    print("✅ Loaded existing model checkpoint!")
except:
    print("⚠️ No saved model found, training from scratch.")

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, save_path=model_path)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Final Model
torch.save(model.state_dict(), model_path)
print("✅ Model saved successfully!")

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Import Required Libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import datasets, models
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from torch.cuda.amp import autocast, GradScaler

# ✅ Clear GPU Memory (If Restarting)
torch.cuda.empty_cache()

# ✅ Define Dataset Path
data_dir = "/content/drive/MyDrive/ct_scans"

# ✅ Image Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# ✅ Load Dataset & Split
full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# ✅ Load DenseNet Model (Optimized to DenseNet121)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)
model.classifier = nn.Linear(model.classifier.in_features, 2)  # 2 classes (Normal, Stroke)
model = model.to(device)

# ✅ Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
scaler = GradScaler()  # Mixed precision training

# ✅ Training Function with Mixed Precision
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, save_path="/content/drive/MyDrive/ctscan_densenet.pth"):
    train_acc_list, val_acc_list = [], []

    for epoch in range(epochs):
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            with autocast():  # Enable Mixed Precision
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        val_acc = evaluate_model(model, val_loader)

        scheduler.step()
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

        # ✅ Save Model Every Epoch (In case of crash)
        torch.save(model.state_dict(), save_path)

    # ✅ Plot Training vs Validation Accuracy
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()
    plt.show()

# ✅ Evaluation Function
def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# ✅ Confusion Matrix & Classification Report
def plot_confusion_matrix(model, val_loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    class_names = full_dataset.classes

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

# ✅ Load Model if Exists (Resume Training)
model_path = "/content/drive/MyDrive/ctscan_densenet.pth"
try:
    model.load_state_dict(torch.load(model_path))
    print("✅ Loaded existing model checkpoint!")
except:
    print("⚠️ No saved model found, training from scratch.")

# ✅ Start Training
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50, save_path=model_path)

# ✅ Plot Confusion Matrix & Classification Report
plot_confusion_matrix(model, val_loader)

# ✅ Save Final Model
torch.save(model.state_dict(), model_path)
print("✅ Model saved successfully!")